{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "470fc6a5-1a33-516e-a3b6-51798eddb9f8",
        "openai_ephemeral_user_id": "143c5970-b006-576d-9fe7-646b1bae4c3f",
        "openai_subdivision1_iso_code": "CA-QC"
      }
    },
    "noteable": {
      "last_transaction_id": "aac727b5-ecab-47ab-bade-1d13346bece6"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "1b31064b-33d6-404d-92c4-14471d388a8a",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "5441fe9f-cbe6-4701-9d01-5ddfaf58c6df"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:29:04.769344+00:00",
          "start_time": "2023-07-06T16:29:00.161106+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install polars==0.18.4 lets-plot==3.2 prince==0.10.7\nimport numpy as np\nimport polars as pl\nimport prince\nfrom lets_plot import *\nLetsPlot.setup_html(isolated_frame=True)",
      "outputs": []
    },
    {
      "id": "7647d78b-950e-45bf-a3c7-ca0e4fa2593a",
      "cell_type": "markdown",
      "source": "# Analyse multivariée 2: ordination\n\nLittéralement, l'ordination vise à mettre de l'*ordre* dans des données, dont le nombre élevé de variables peut amener à des difficultés d'appréciation et d'interprétaion ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)). En science des données, le terme ordination est utilisé pour désigner un ensemble de techniques de réduction d'axe. L'analyse en composante principale est probablement la plus connue de ces techniques, mais de nombreuses techniques d'ordination ont été développées au cours des dernières années, chacune ayant ses domaines d'application.\n\nLes techniques de réduction d'axe permettent de dégager l'information la plus importante en projetant une synthèse des relations entre les observations et entre les variables. Les techniques ne supposant aucune structure a priori sont dites *non contraignantes*. À l'inverse, les ordinations contraignantes lient des variables descriptives avec une ou plusieurs variables prédictives.\n\nLa référence en la matière est indiscutablement [Legendre et Legendre (2012)](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0). Je m'inspirerai toutefois de l'approche du module [*Prince*](https://maxhalford.github.io/prince/), qui offre des fonctions pour l'ordination non contraignante. Dans cette section, nous ne couvrirons que l'analyse en composantes principales et l'analyse discriminante.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "dfb49540-aca5-4a09-9fd7-5ce052b3923e",
      "cell_type": "markdown",
      "source": "![](images/ord_flow-chart.png)\n\n<small>Arbre de décision pour l'ordination, inspiré de [Max Halford](https://maxhalford.github.io/prince/)</small> ",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "ebdc008f-f207-411f-8e9f-d2a14ee484c9",
      "cell_type": "markdown",
      "source": "Le module *Prince* vient avec des méthodes pour visualiser les résultats des ordinations. Jusqu'à présent, nous avons utilisé [*Lets-Plot*](https://lets-plot.org/), mais *Prince* utilise plutôt [*Altair*](https://altair-viz.github.io/) comme module graphique. Comme *Lets-Plot*, *Altair* fonctionne en mode de visualisation déclarative. Le vocabulaire est différent, mais les principes sont sensiblement les mêmes.\n\nPlusieurs modules en Python peuvent effectuer ces opérations d'ordination.\n\n- Analyse en composantes principales: [Prince](https://maxhalford.github.io/prince/pca/), [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) et [Statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.multivariate.pca.PCA.html).\n- Analyse procustéenne généralisée: [Prince](https://maxhalford.github.io/prince/gpa/).\n- Analyse factorielle multiple: [Prince](https://maxhalford.github.io/prince/mfa/).\n- Analyse de correspondance: [Prince](https://maxhalford.github.io/prince/ca/).\n- Analyse de correspondance multiple: [Prince](https://maxhalford.github.io/prince/mfa/).\n- Analyse de factorielle de données mixes: [Prince](https://maxhalford.github.io/prince/famd/).\n- Échelle multidimensionnelle: [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS).\n- Analyse de corrélations canoniques: [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA)\n- Analyse discriminante: [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis).\n",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "5231740a-67f5-457c-aba9-66bdda3cd3ba",
      "cell_type": "markdown",
      "source": "## Analyse en composantes principales (ACP)\n\nL'objectif d'une ACP est de représenter les données dans un nombre réduit de dimensions représentant le plus possible la variation d'un tableau de données : elle permet de projeter les données dans un espace dans lequel les variables sont combinées en axes orthogonaux dont le premier axe capte le maximum de variance. L'ACP peut par exemple être utilisée pour analyser des corrélations entre variables ou dégager l'information la plus pertinente d'un tableau de données météo ou de signal en un nombre plus retreint de variables.\n\nL'ACP effectue une rotation des axes à partir de sorte que le premier axe définisse la direction à travers laquelle on retrouve la variance maximale. Ce premier axe est une combinaison linéaire des variables et forme la première composante principale. Une fois cet axe définit, on trouve le deuxième axe, orthogonal au premier, où l'on retrouve la variance maximale — cet axe forme la deuxième composante principale, et ainsi de suite jusqu'à ce que le nombre d'axes corresponde au nombre de variables.\n\nLes projections des observations sur ces axes principaux sont appelées les **scores**. Les projections des variables sur les axes principaux sont les **vecteurs propres**  (*eigenvectors*, ou *loadings*). La variance des composantes principales diminue de la première à la dernière, et peut être calculée comme une proportion de la variance totale : c'est le **pourcentage d'inertie**. Par convention, on utilise les **valeurs propres** (*eigenvalues*) pour mesurer l'importance des axes. Si la première composante principale a une inertie de 50% et la deuxième a une inertie de 30%, la représentation en 2D des projections représentera 50% + 30% = 80% de la variance du nuage de points.\n\nL'hétérogénéité des échelles de mesure peut avoir une grande importance sur les résultats d'une ACP (les données doivent être dimensionnellement homogènes). En effet, la hauteur d'un cerisier aura une variance plus grande que le diamètre d'une cerise exprimé dans les mêmes unités, et cette dernière aura plus de variance que la teneur en cuivre d'une feuille. Il est conséquemment avisé d'homogénéiser les échelles en centrant la moyenne à zéro et l'écart-type à 1 avant de procéder à une ACP.\n\nL'ACP a été conçue pour projeter en un nombre moindre de dimensions des observations dont les distributions sont multinormales. Bien que l'ACP soit une technique robuste, il est préférable de transformer préalablement les variables dont la distribution est particulièrement asymétrique ([Legendre et Legendre, 2012, p. 450](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)). Le cas échéant, les valeurs extrêmes pourraient faire dévier les vecteurs propres et biaiser l'analyse. En particulier, les données ACP menées sur des données compositionnelles sont réputées pour générer des analyses biaisées ([Pawlowsky-Glahn and Egozcue, 2006](http://sp.lyellcollection.org/content/specpubgsl/264/1/1.full.pdf)). Le test de Mardia ([Korkmaz, 2014](https://journal.r-project.org/archive/2014-2/korkmaz-goksuluk-zararsiz.pdf)) peut être utilisé pour tester la multinormalité. Une distribution multinormale devrait générer des scores en forme d'hypersphère (en forme de cercle sur un biplot: voir plus loin).\n\nVoyons ce que nous dira une ACP sur nos données de manchots.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "35aa2297-1ee5-4133-97ec-fed67b8736dd",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "bc14ea5d-8823-447f-b92d-db58bb215500"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:29:04.985704+00:00",
          "start_time": "2023-07-06T16:29:04.825912+00:00"
        },
        "datalink": {
          "782e676e-bc7e-4f1b-a043-65b98afc510d": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 9,
              "orig_num_rows": 344,
              "orig_size_bytes": 27520,
              "truncated_num_cols": 9,
              "truncated_num_rows": 344,
              "truncated_size_bytes": 27520,
              "truncated_string_columns": []
            },
            "display_id": "782e676e-bc7e-4f1b-a043-65b98afc510d",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-06-22T19:45:42.748679",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_2fbddf02216046a5be1c25b6c5d084c5"
          },
          "41d7a5a8-2778-41fd-acc1-ba3fa159e399": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 9,
              "orig_num_rows": 344,
              "orig_size_bytes": 27520,
              "truncated_num_cols": 9,
              "truncated_num_rows": 344,
              "truncated_size_bytes": 27520,
              "truncated_string_columns": []
            },
            "display_id": "41d7a5a8-2778-41fd-acc1-ba3fa159e399",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-06-22T19:46:27.635149",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_6f722d7a21a1402483dcfd928b3c046d"
          }
        }
      },
      "execution_count": null,
      "source": "penguins = pl.read_csv('data/penguins.csv', null_values='NA')\npenguins = (\n    penguins\n    .filter(pl.all(pl.col(pl.Float32, pl.Float64).is_not_nan())) # enlever les NaN\n    .drop_nulls() # enlever les None\n)",
      "outputs": []
    },
    {
      "id": "1b4de67f-e4f0-4f40-be3a-a84a180a35d4",
      "cell_type": "markdown",
      "source": "J'ai pris la liberté de nettoyer le tableau `penguins` en enlevant les valeurs NaN (*not a number*) et les valeurs *None*. L'ACP est généralement effectuée sur les *features* (variables prédictives). J'inclus le nom des *features* de mon tableau dans une liste pour la convivialité. Je prends cette liste pour créer un nouveau tableau. Puisque le module *Prince* n'accepte que les tableaux de type *Pandas*, j'effectue la conversion. Prenez note qu'un tableau *Pandas* ne possède pas les mêmes propriétés qu'un tableau *Polars*.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "f784d201-2b87-4d88-ac5a-5bd246bd169e",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "597fe884-3626-4a50-875b-9a4ed3cfbe79"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:29:05.184844+00:00",
          "start_time": "2023-07-06T16:29:05.024756+00:00"
        }
      },
      "execution_count": null,
      "source": "ordination_features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\npenguins_ord_df = penguins.select(ordination_features).to_pandas()",
      "outputs": []
    },
    {
      "id": "aea767f7-2738-4b86-a4e5-affe264b6fd8",
      "cell_type": "markdown",
      "source": "*Prince* fonctionne comme *Scikit-Learn* (élaboré dans la prochaine section): instanciation de l'objet (l'objet étant le modèle), puis lissage sur les données.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "e7d2d986-0432-432d-abf4-82f1c9f78379",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4e124fe0-4545-4604-9b17-b793e5c88ac6"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:29:05.468083+00:00",
          "start_time": "2023-07-06T16:29:05.250702+00:00"
        },
        "datalink": {
          "3d7ff123-1701-4ded-9448-011b7ec1d5d6": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 3,
              "orig_num_rows": 4,
              "orig_size_bytes": 128,
              "truncated_num_cols": 3,
              "truncated_num_rows": 4,
              "truncated_size_bytes": 128,
              "truncated_string_columns": []
            },
            "display_id": "3d7ff123-1701-4ded-9448-011b7ec1d5d6",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-06-28T14:26:07.661785",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_81f434185ffd46f5ba038ca4cd4e9401"
          }
        }
      },
      "execution_count": null,
      "source": "# instanciation de l'objet\npenguins_pca = prince.PCA(\n    n_components=len(ordination_features),\n    n_iter=10,\n    rescale_with_mean=True,\n    rescale_with_std=True,\n    copy=True,\n    check_input=True,\n    engine='sklearn'\n)\n\n#lissage\n## to_pandas parce que Prince ne prend pas les tableaux Polars, mais Pandas\npenguins_pca.fit(penguins_ord_df) ",
      "outputs": []
    },
    {
      "id": "47fba623-628d-4492-8166-bb7614f7d13f",
      "cell_type": "markdown",
      "source": "L'objet lissé (ma traduction de *fitted*) contient toutes les informations nécessaires pour interpréter l'ACP. Le sommaire des valeurs propres permet de connaître le pourcentage cumulatif de la variance captée par les composantes, un indicateur important pour interpréter un biplot (voir plus loin) ou pour décider si l'information des variables peut être condensée en un nombre moindre de variables composites. On peut aller chercher les valeurs propres dans le sommaire par `penguins_pca.eigenvalues_summary`. Ou bien, en lançant la visualisation des valeurs propres. Le graphique étant interactif, vous verrez davantage d'informations et plaçant le curseur sur les barres.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "7d870fa0-5a00-47c7-8c1c-f19a50474575",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "707746fd-06be-4773-9f71-8343e907271e"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:29:05.776198+00:00",
          "start_time": "2023-07-06T16:29:05.525462+00:00"
        }
      },
      "execution_count": null,
      "source": "penguins_pca.scree_plot()",
      "outputs": []
    },
    {
      "id": "e33d5a73-216b-4b76-8d09-a2a6a3b491c7",
      "cell_type": "markdown",
      "source": "L'objet `penguins_pca` contient aussi les informations brutes:\n\n- valeurs propres\n- les scores, qui représentent les observations\n- loadings, qui représentent les variables ",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "5e10e1a1-864f-4d62-8177-9ad1fb802fa9",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "python",
          "output_collection_id": "5b891bbb-4722-4fce-9df8-72d7318fa01e"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:29:05.989197+00:00",
          "start_time": "2023-07-06T16:29:05.829899+00:00"
        }
      },
      "execution_count": null,
      "source": "# les valeurs propres\npenguins_ev = penguins_pca.eigenvalues_ \n\n# les scores\npenguins_scores = penguins_pca.transform(penguins_ord_df)\n\n# les loadings\npenguins_loadings = penguins_pca.column_coordinates_",
      "outputs": []
    },
    {
      "id": "596278fe-ebe8-46a7-8d9f-4124c40c8b8f",
      "cell_type": "markdown",
      "source": "### Représentation en biplot\n\nLes scores et les loadings permettent de représenter les données sous forme de biplot. Mais, préalablement, [Borcard et al. 2011](https://link.springer.com/article/10.1007/s13253-012-0094-x) proposent d'effectuer consciencieusement une mise à l'échelle (*scaling*). En gros, vous préférerez le **scaling 1** pour un biplot de distance, où la distance entre les objets (observations et variables) dans le biplot est proche des distances entre les objets dans les données de haute dimension. Vous préférerez le **scaling 2** pour les biplots de corrélation, où les angles entre les chargements (avec des segments reliant les chargements à l'origine) sont proches de leur corrélation (lisez [Borcard et al. 2011](https://link.springer.com/article/10.1007/s13253-012-0094-x) pour plus de détails). À ma connaissance, les autres échelles sont rarement utilisées. De plus, bien que peu d'articles scientifiques se soucient de l'échelle, elle affecte considérablement l'interprétation: mieux vaut spécifier le scaling lorsque vous présentez un biplot.\n",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "a0843a8c-4cfa-4cb4-b1fa-f61881ed8dba",
      "cell_type": "markdown",
      "source": "![](images/pca_scaling.png)\n\nCapture d'écran de https://rdrr.io/rforge/vegan/f/inst/doc/decision-vegan.pdf",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "d12dd301-54d1-4d6c-9029-4664f320f6f8",
      "cell_type": "markdown",
      "source": "La fonction suivante effectue le scaling.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "4f6a4ef2-c976-488b-a899-d6d2780fba78",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "62320d49-a95f-474f-9f8e-4939ae85acf4"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:29:37.183783+00:00",
          "start_time": "2023-07-06T16:29:37.020461+00:00"
        }
      },
      "execution_count": null,
      "source": "def eigen_scaling(scores, loadings, eigenvalues, scaling = 0):\n    \"\"\"\n    pca is a PCA object obtained from statsmodels.multivariate.pca\n    scaling is one of [0, 1, 2, 3]\n    the eigenvalues of the pca object are n times the ones computed with R\n    we thus need to divide their sum by the number of rows\n    \"\"\"\n   \n    const = ((scores.shape[0]-1) * eigenvalues.sum()/ scores.shape[0])**0.25\n    if scaling == 0:\n        scores = scores\n        loadings = loadings\n    elif scaling == 1:\n        scaling_fac = (eigenvalues / eigenvalues.sum())**0.5\n        scores = scores * scaling_fac * const\n        loadings = loadings * const\n    elif scaling == 2:\n        scaling_fac = (eigenvalues / eigenvalues.sum())**0.5\n        scores = scores * const\n        loadings = loadings * scaling_fac * const\n    elif scaling == 3:\n        scaling_fac = (eigenvalues / eigenvalues.sum())**0.25\n        scores = scores * scaling_fac * const\n        loadings = loadings * scaling_fac * const\n    else:\n        sys.exit(\"Scaling should either be 0, 1, 2 or 3\")\n    return([scores, loadings])\n",
      "outputs": []
    },
    {
      "id": "5555a5a1-567f-4ad6-a8b7-b2f3e0656faa",
      "cell_type": "markdown",
      "source": "J'effectue un `scaling = 2` pour avoir une idée des corrélations entre les variables.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "7c0b7a9e-707b-4427-9021-586fdb9d8d06",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "31ffc243-1c14-4f52-bf95-c48015668a49"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:29:40.679499+00:00",
          "start_time": "2023-07-06T16:29:40.520430+00:00"
        }
      },
      "execution_count": null,
      "source": "penguins_scores_sc, penguins_loadings_sc = eigen_scaling(\n    scores = penguins_scores,\n    loadings = penguins_loadings,\n    eigenvalues = penguins_ev,\n    scaling = 2\n)",
      "outputs": []
    },
    {
      "id": "f28dbeac-4934-4284-bab8-7110f37472b6",
      "cell_type": "markdown",
      "source": "Les entêtes des tableaux des scores et des loadings sont des valeurs numériques, ce qui complique leur utilisation subséquente pour les graphiques avec *Lets-Plot*. Les manipulations de la prochaine cellule consistent à transformer les tableaux de sortie de *Prince* en format *Polars*, à changer le nom des colonnes par PC0, PC1, PC2 et PC3. Pour les scores, j'ajoute une colonne identifiant l'espèce. Enfin, puis les loadings, j'ajoute une colonne identifiant l'attribut (*feature*).",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "d611c21f-a196-44da-a1ad-0ef1fb71ef6a",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "python",
          "output_collection_id": "b859b453-53d2-4382-a566-a04c8708048f"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:29:44.194569+00:00",
          "start_time": "2023-07-06T16:29:44.034361+00:00"
        }
      },
      "execution_count": null,
      "source": "pca_colnames = ['PC' + str(i) for i in range(len(ordination_features))]\npenguins_scores_sc = (\n    pl.DataFrame(penguins_scores_sc, schema = pca_colnames)\n    .with_columns(penguins.select('species'))\n)\npenguins_loadings_sc = (\n    pl.DataFrame(penguins_loadings_sc, schema = pca_colnames)\n    .with_columns(pl.Series(ordination_features).alias('feature'))\n)",
      "outputs": []
    },
    {
      "id": "8b22d4c5-2c83-46e2-be0a-15e12878078f",
      "cell_type": "markdown",
      "source": "Nos tableaux nous permettent de créer un biplot avec *Lets-Plot*.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "0d6aa13b-ec7a-45b4-8bb4-c9c0b2649882",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "477bdfd3-d7ce-4db3-b293-b303c7be1199"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:30:15.199916+00:00",
          "start_time": "2023-07-06T16:30:15.019882+00:00"
        }
      },
      "execution_count": null,
      "source": "(\n    ggplot()\n    + geom_point(\n        data = penguins_scores_sc,\n        mapping = aes(x='PC0', y='PC1', color='species')\n    )\n    + geom_segment(\n        data = penguins_loadings_sc,\n        mapping = aes(xend = 'PC0', yend = 'PC1'),\n        x = 0, y = 0, color = 'black'\n    )\n    + geom_text(\n        data = penguins_loadings_sc,\n        mapping = aes(x = 'PC0', y = 'PC1', label = 'feature')\n    )\n)",
      "outputs": []
    },
    {
      "id": "f9f93923-766f-4c14-8c4e-91b733a9ea4f",
      "cell_type": "markdown",
      "source": "La représentation en biplot est ce qu'on retrouverait si on écrasait en 2D les données selon la perspective où les données sont le plus éclatées. Voici quelques interprétations du biplot:\n\n- Étant donnée qu'on est en scaling 2, nous pouvons conclure que la longueur des ailes et la masse du manchot sont positivement corrélés. Ces deux attributs sont positivement corrélés, mais dans une moindre mesure, avec la longueur du bec, mais négativement corrélés avec l'épaisseur du bec.\n- Les Gentoos sont différents des Adelie et Chinstrap, qui se ressemblent davantage. Les Gentoos se différencient des autres par leur masse plus élevée, leurs ailes plus longues, et leur bec plus long. Les Adelies et des Chinstraps se recoupent passablement, mais peuvent se différencier selon la longueur de leur bec.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "022bd328-5060-47cb-845e-b3830f9dc36c",
      "cell_type": "markdown",
      "source": "![](images/Penguin-heights.jpg)\n\nSource: https://www.bas.ac.uk/about/antarctica/wildlife/penguins/",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "ec19c2e3-62f5-48ce-bffe-af04321d749d",
      "cell_type": "markdown",
      "source": "## Analyse discriminante (DA)\n\nL'analyse discriminante est une technique d'ordination utilisée pour distinguer ou prédire des catégories. Elle cherche à trouver les combinaisons de variables qui permettent de séparer au mieux les différentes catégories.\n\nNous allons utiliser la classe `LinearDiscriminantAnalysis` de `scikit-learn` pour effectuer une analyse discriminante sur un ensemble de données.\n\nNous devons d'abord convertir la colonne `species` en valeurs numériques.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "e40d7478-91ff-4d13-a800-55f6a9e8ee8a",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "python",
          "output_collection_id": "d83ce604-24af-45cc-8951-0647cf6e7f37"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:30:17.575856+00:00",
          "start_time": "2023-07-06T16:30:17.414444+00:00"
        }
      },
      "execution_count": null,
      "source": "penguins = (\n    penguins\n    .with_columns(pl.col('species').cast(pl.Categorical).cast(pl.UInt32).alias('species_int'))\n)",
      "outputs": []
    },
    {
      "id": "6613f671-f7b8-4ec8-8381-ef535f19f256",
      "cell_type": "markdown",
      "source": "À ce stade, je vous laisse interpréter mon code!",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "eaba15e7-def8-4076-be18-aaa6ec9dd46c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ee43dae6-26ec-44f0-a85d-f25ffc6f0d2b"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:31:07.785194+00:00",
          "start_time": "2023-07-06T16:31:07.620431+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\npenguins_sc = scaler.fit_transform(penguins.select(ordination_features))\n\n# Analyse discriminante\npenguins_lda = LDA(n_components=2)\npenguins_lda.fit(\n    penguins_sc,\n    penguins.select('species_int')\n)\n\n# L'extraction des attributs est différente qu'avec Prince\nlda_scores = penguins_lda.transform(penguins_sc)\nlda_loadings = penguins_lda.coef_.T[:, :lda_scores.shape[1]]\nlda_eigenvalues = penguins_lda.explained_variance_ratio_",
      "outputs": []
    },
    {
      "id": "8061d06e-f534-4973-8520-3fae417faf39",
      "cell_type": "markdown",
      "source": "Notez que `penguins_lda.explained_variance_ratio_` extrait un ratio, qu'il faudrait multiplier par la variance `penguins.select(ordination_features).std()**2`, mais celle-ci est unitaire étant donné que l'on a préalablement mis les données à l'échelle avec `StandardScaler()`.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "8bb3df1d-717a-4262-b8f6-e667187cdea6",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "efd5aeb8-8256-4f48-bcaf-861200bff0a7"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:32:51.909740+00:00",
          "start_time": "2023-07-06T16:32:51.751178+00:00"
        }
      },
      "execution_count": null,
      "source": "# Scaling\npenguins_lda_scores, penguins_lda_loadings = eigen_scaling(\n    scores = lda_scores,\n    loadings = lda_loadings,\n    eigenvalues = lda_eigenvalues,\n    scaling = 2\n)\n\n# Transformer en format Polars, et renommer les variables\nlda_colnames = ['DA0', 'DA1']\npenguins_lda_scores = (\n    pl.DataFrame(penguins_lda_scores, schema = lda_colnames)\n    .with_columns(penguins.select('species'))\n)\npenguins_lda_loadings = (\n    pl.DataFrame(penguins_lda_loadings)\n    .with_columns(pl.Series(ordination_features).alias('features'))\n)",
      "outputs": []
    },
    {
      "id": "a4b0fdb0-1997-4031-96c9-1396afb74dca",
      "cell_type": "markdown",
      "source": "Et le biplot en scaling 2!",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "4502ff35-a6cd-4c2d-938b-e1d7d1059839",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "147b32ba-380b-4832-9cfd-a4ce6d0d954c"
        },
        "ExecuteTime": {
          "end_time": "2023-07-06T16:34:00.914098+00:00",
          "start_time": "2023-07-06T16:34:00.740109+00:00"
        }
      },
      "execution_count": null,
      "source": "(\n    ggplot(data=penguins_lda_scores)\n    + geom_point(aes(x='DA0', y='DA1', color='species'))\n    + geom_segment(\n        data = penguins_lda_loadings,\n        mapping = aes(xend = 'column_0', yend='column_1'),\n        x = 0, y = 0, color = 'black'\n    )\n    + geom_text(\n        data = penguins_lda_loadings,\n        mapping = aes(x = 'column_0', y = 'column_1', label='features')\n    )\n)",
      "outputs": []
    },
    {
      "id": "e6422943-2a0a-4a52-8985-6b29111e879a",
      "cell_type": "markdown",
      "source": "À la différence d'une analyse en composantes principales, l'analyse discriminante écrase en 2D la perspective où les groupes sont les plus éclatés. Pouvez-vous interpréter le biplot?",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    }
  ]
}